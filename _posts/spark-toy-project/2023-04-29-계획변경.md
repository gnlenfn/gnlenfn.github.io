---
title: "토이 프로젝트 진행 상황과 난관"
excerpt: "spark 토이 프로젝트"

categories:
  - Data Enginnering
tags:
  - ["Spark", "Python", "Toy project"]

date_created: 2023-04-29T16:27:35+09:00
last_modified_at: 2023-04-92T16:27:59+09:00
toc: true
toc_sticky: true
---

# Spark를 활용한 데이터 파이프라인 구축 연습

최근 스파크 관련 강의를 들으면서 연습을 위해 스파크를 넣은 데이터 파이프라인을 구축해보는 토이 프로젝트를 시작하였다. 그리고 이제 일주일정도 지났는데...
![](/assets/img/2023-04-29-toy-project/pipeline.drawio.svg)

처음 계획은 이렇게 유튜브와 트위치의 라이브 방송에서 채팅 데이터를 수집하고 Google Cloud Storage에 저장한 후 이후 ETL 작업을 하는 방향으로 계획을 했다. (그림에는 S3를 해놨는데, GCP 무료가 한 달 정도 남아서 그걸 쓰기로 했다)
하지만 일주일 동안 유튜브 라이브 채팅 수집도 제대로 진행이 안되고 있다. 

## Youtube
정확한 원인을 찾지 못하고 있어서 더 답답한 상황이다. 우선 유튜브와 트위치는 API로 실시간 채팅을 수집할 수 없다. 그런 API는 제공하지 않고 있다. 유튜브의 경우 pychat이라는 라이브러리를 통해 라이브 채팅을 수집할 수 있어서 해당 라이브러리를 사용해 수집 코드를 작성했고 GCS 적재까지 잘 되는 것을 확인했다. 간단하게 100개 정도 채팅 적재하는 것만 테스트했지 실제로는 하루에 한번 parquet를 저장할 수 있도록 했다. 
그런데 어떤 이유에서인지 약 1 ~ 2시간 정도 수집을 하면 프로세스가 종료된다. 에러는 나오는데 h2라는 라이브러리가 커넥션 에러를 뱉는데, 어디서 쓰는건지 모른다..
예상하기로는 pychat이 유튜브 스트리밍 비디오와 계속해서 연결이 지속되면서 과도한 사용량으로 차단이 된다던가 하는 이유인 것으로 추정중이다. 그렇다면 채팅 수집을 어떻게 할 지 부터 시작해야 하는데, 애초에 프로젝트 목적이 채팅 수집이 아니기 때문에 거기에 시간을 쏟기 보다는 다른 걸 하는게 낫겠다고 생각했다. 

## Twitch
트위치의 경우 더욱 상황이 좋지 않다. API에서는 당연히 실시간 방송의 채팅 수집을 제공하지 않고 그나마 다시보기로 남은 영상의 채팅을 수집할 수 있는 API가 있다. 또한 Github에서도 다시보기 영상의 채팅만 추출해서 저장할 수 있도록 하는 오픈소스가 있었다. 하지만 작년 말 망사용료 이슈와 함께 트위치에서는 한국의 스트리밍 화질 제한과 클립, 다시보기 등 트래픽이 많이 발생할 만한 서비스들을 모두 중단했다. 그래서 지금 다시보기 영상을 구할 수 없는 상황이다. (한국 방송 뿐 아니라 외국 방송도 한국에서는 다시보기가 불가능하다)

## 다른 데이터?
그래서 실시간 방송의 채팅 수집 말고 다른 데이터를 사용하는 방향으로 계획을 변경하려고 한다.
연초에 했던 데이터 엔지니어 줌 캠프에서 봤던 [뉴욕 택시 데이터](https://www.slideshare.net/AlexeyGrigorev/data-engineering-zoomcamp-introduction)를 사용하는 것이 어떨까 생각하고 있다. 데이터양도 꽤 많아서 빅데이터 흉내내기에도 괜찮지 않을까 생각한다. 
다만 이 데이터는 parquet를 다운 받아서 GCS로 업로드하는 코드는 있겠지만 그럼 이걸로 kafka를 활용한 스트리밍은 어떻게 쓰는지 고민을 해봐야 할 것 같다. 내가 스트리밍 환경에 대해 잘못 이해하고 있는건지.. 

일주일 동안 유튜브 채팅 데이터 수집에서 저장하겠다고 열심히 했는데 생각보다 제한 사항이 많았던 것 같다. 물론 데이터 엔지니어에게 수집 작업도 매우 중요한 업무일 수 있지만 우선은 이 프로젝트의 목표가 채팅 수집이 목표는 아니기 때문에 데이터 수집 부분은 했다 치고 그 뒷 부분에 좀 더 집중해보려고 한다.
