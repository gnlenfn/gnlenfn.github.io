---
title: "Spark로 MySQL에 데이터 저장하기"
excerpt: "spark 토이 프로젝트"

categories:
  - Data Enginnering
tags:
  - ["Spark", "Python", "Toy project"]

date_created: 2023-05-09T16:27:35+09:00
last_modified_at: 2023-05-09T16:27:59+09:00
toc: true
toc_sticky: true
---

# Spark를 사용한 데이터 파이프라인 구축

첫 일주일 간 유튜브 라이브 채팅 수집을 시도하다가 짧은 시간 동안은 수집이 가능한데, 한 시간 이상 수집을 지속하면 연결이 끊어지는 문제가 계속 발생했다. 코드의 구조상 계속해서 API를 날려서 차단되는 느낌을 받았다. 그래서 일단 빠르게 Spark 프로그래밍을 해보기 위해 이미 구축된 데이터를 사용하기로 했었다.

## 파이프라인 구조
![](/assets/img/spark%20toy%20project/2023-05-09/pipeline2.drawio.png)

우선은 아주 간단한 파이프라인이다. NYC 택시 데이터를 홈페이지에서 다운받고 Google Cloud Storage로 업로드 한다. 그리고 그 데이터를 Spark를 통해 월별 통계를 낸 후 MySQL 테이블로 저장하도록 했다.

## Spark Programming
이 토이 프로젝트 자체가 스파크를 사용해보고 공부하기 위해 시작한 것이다. 그리고 간단하게나마 스파크 프로그래밍을 해본 소회랄까..? 중간 발표 같은 내용이다 ㅋㅋ

### Spark는 신이고 SQL은 무적이다
사실 SQL이 중요하다 중요하다 하지만 정형화된 데이터보다는 비정형 데이터를 보아왔던 입장에서 SQL을 접할 일이 크게 없었고 그냥 NoSQL 쓰면 되는거아닌가? 생각도 했었다. 그러나 고전은 오랫동안 사랑받는 이유가 있는법. 정형화된 데이터를 다루는데 있어서 스파크에서도 데이터 프레임 보다는 SQL이 훨씬더 간편하고 알아보기 쉽다는 점을 느꼈다. 
그리고 SQL은 꼭 개발자만 쓰는 것이 아니기 때문에 다른 직군의 사람들과 함께 일을 한다면? pyspark 코드를 사용하는 것 보다는 훨씬 서로 알아보기 쉬울 것이라고 생각한다. 
하지만 나의 SQL 쿼리 짜는 실력이 아직 많이 부족하고 아이디어가 별로 없어서 많은 데이터를 저장하지는 못했다. 하지만 SQL을 쓰는 것이 데이터프레임보다 편했던 점은 같은 작업을 하더라도 SQL은 쿼리 하나로 끝나기 때문에 전체 길이가 짧아질 수 있다는 장점이 있었다. 그리고 짧은에도 불구하고 가독성 역시 뛰어났다.
SQL 공부도 열심히 해야할 것 같다.

### Spark Job, Stage, Task
우선 사용한 데이터가 그렇게 큰 데이터가 아니기 때문에 `spark-submit` 실행 후 여유있게 웹 UI에서 시각화 된 것을 확인하는건 어려웠기 때문에 프로그램이 종료되지 않도록 멈춘 후 확인하도록 했다. 
> Job: 하나 이상의 Stage로 구성, 전체 작업의 시작부터 끝까지. 가장 큰 단위를 가리킴   
Stage: task가 모여 구성된 DAG 형태의 작업 모음, task를 병렬 실행 가능, shuffle이 발생할 때 새로운 Stage가 생긴다.  
Task: 가장 작은 실행 유닛으로 executor가 실행 시키는 작업

스파크의 작업은 위 세 가지의 구성요소로 되어있다. 
![](/assets/img/spark%20toy%20project/2023-05-09/jobs.png)
![](/assets/img/spark%20toy%20project/2023-05-09/stages.png)
![](/assets/img/spark%20toy%20project/2023-05-09/tasks.png)

내가 작성한 프로그램은 총 4개의 Job이 존재하고 각각 4개의 stage로 구성되어있다. 그리고 각 stage는 웹UI를 통해 시각화 된 것을 볼 수 있다. 
우선 스파크에 내재된 최적화 알고리즘을 통해 작업을 최적화 시켜주는 것이 있다고 한다. 하지만 공부하는 입장에서는 최대한 shuffle이 적게 일어나도록 프로그램을 작성하는 것이 최적화이 비결이라는 것을 배웠다.
새로운 파일을 읽거나, JOIN이 일어나는 등의 행위에서 shuffle이 일어나기 때문에 (내 코드는 대부분 파일 읽는데 shuffle이 생겼다) 이 부분을 잘 고려해서 코드를 작성하는 것이 필요할 것 같다.

### 기타 트러블 슈팅
간단하게 짚어볼 트러블 슈팅으로는...
1. 날짜표현
스파크 데이터프레임으로 날짜를 처리할 때 파이썬에서 쓰던 날짜표현 문자열과 조금 다르다는 것을 발견할 수 있었다. (스파크 3.4 버전 가이드)[https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html]에서는 월 표현을 `M`으로 했는데, 파이썬 `datetime` 패키지 사용 시 `%m`으로 했었기 때문이다. 기억해야 할 부분인 것 같다.

2. MySQL 8.0 useSSL
MySQL에 데이터를 저장하기 위해 도커 컨테이너로 간단하게 MySQL을 실행시켰다. 그리고 크게 생각 없이 최신 버전으로 사용했는데, MySQL 커넥터 jar를 사용해도 접속이 안되는 이슈가 있었다. 알고보니 8.0 버전 부터는 url, username, password 외에 SSL 접속을 위한 키가 필요하다고 한다. 그래서 간단한 프로젝트이기 때문에 SSL 사용을 비활성화 하는 작업이 추가적으로 필요했다. 
```
jdbc:mysql://localhost:3306/test_db?useSSL=false&allowPublicKeyRetrieval=true
```
MySQL 접속 URI에 파라미터로 두 가지를 추가시켜주니 정상적으로 username과 password만으로 접속이 가능했다.

3. 스파크 프로그램 실행시키기
스파크를 이번에는 간단하게 로컬 StandAlone 방식으로 사용했다. 그리고 스파크를 사용하는 방법으로는 스파크 콘솔을 열어 직접 코드를 작성하는 방법이 있고, 파이썬 파일로 작성하여 실행시키는 방식이 있다. 주피터 노트북이나 제플린 등을 사용하여 대화하듯 사용할 수도 있다. 
처음 프로젝트를 진행하면서 별 생각없이 vscode의 파이썬 파일 실행을 시키면서 동작을 확인했었다. 그런데 이 스파크 Job을 Airflow로 스케쥴링 할 생각을 해보니 어떤 Operator를 써야하는 건지 헷갈렸다.
BashOperator를 써도 그냥 `python target_file.py` 처럼 실행시키는게 맞나 싶었는데.. 결론은 `spark-submit`을 쓴다는 것이다. Airflow Operator 중에서도 `SparkSubmitOperator`가 있고 BashOperator를 쓰더라도 `spark-submit`을 통해 실행시키는 것이 맞다고 한다. 
아직 확실하게 찾아본 것은 아니지만 지금은 local StandAlone 모드이기 때문에 그냥 파이썬 파일 실행으로도 같은 결과를 얻을 수 있지만 클러스터로 구성된다면 `spark-submit`을 사용해야 클러스터의 워커로도 명령을 전달할 수 있을 것이라고 예상한다.

## 프로젝트 마무리까지..
중간 평가로 작성해보았는데, 이제 spark job을 Airflow로 시간마다 실행시키도록 스케쥴링 할 게획이다. 그리고 지금은 GCS로부터 parquet 파일을 모두 다운받아 실행시키고 있는데, 다운 받지 않고 직접 읽어서 사용 가능한지도 알아 볼 것이다. 